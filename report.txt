general hints:

all programs can be run like this to get instructions:

$ python3 assignment2.py

Current script file is:  assignment2.py
How to use this program?
Run this program with parameter: (one at a time)

1 - generate email training file (spam training file)
2 - convert email test files into Project Data format without labels
3 TRAININGFILE PERCENTAGE - generate, say, 75% of labeled data for training and the rest for testing
4 - generate naive Bayes spam model
5 - generate naive Bayes sentiment model



1. I randomly selected 75% of the labeled development data for training, and the rest for testing.

For spam/ham test, I used "naive Bayes" and got

precision of positive:  0.989657631954351
recall of positive:  0.9764250527797326
F1 score of positive:  0.9829968119022316

precision of negative:  0.975609756097561
recall of negative:  0.975609756097561
F1 score of negative:  0.975609756097561

For sentiment test,

precision of positive:  0.8355869698832207
recall of positive:  0.8748391248391248
F1 score of positive:  0.8547626532536938

precision of negative:  0.8701602136181575
recall of negative:  0.8701602136181575
F1 score of negative:  0.8701602136181574

SVM-Light toolkit:

spam test:

precision of positive:  0.9255499153976311
recall of positive:  0.9941839331152308
F1 score of positive:  0.958640028040659

precision of negative:  0.9938366718027735
recall of negative:  0.9938366718027735
F1 score of negative:  0.9938366718027735

sentiment test:

precision of positive:  0.8848966613672496
recall of positive:  0.8597466790237874
F1 score of positive:  0.8721403948605453

precision of negative:  0.8537842190016103
recall of negative:  0.8537842190016103
F1 score of negative:  0.8537842190016103


MegaM toolkit:

spam test:

precision of positive:  0.9974554707379135
recall of positive:  0.9920462762111352
F1 score of positive:  0.9947435200290013

precision of negative:  0.9921428571428571
recall of negative:  0.9921428571428571
F1 score of negative:  0.9921428571428571

sentiment test:

precision of positive:  0.9386327503974563
recall of positive:  0.921635966281611
F1 score of positive:  0.9300567107750474

precision of negative:  0.9191626409017714
recall of negative:  0.9191626409017714
F1 score of negative:  0.9191626409017714


2. I randomly selected 25% of the labeled development data for training, and the rest 75% for testing.

For spam/ham test, I used "naive Bayes" and got

precision of positive:  0.9856661045531198
recall of positive:  0.9749791492910759
F1 score of positive:  0.980293501048218

precision of negative:  0.9748533109807209
recall of negative:  0.9748533109807209
F1 score of negative:  0.9748533109807209

For sentiment test,

precision of positive:  0.8178468280051787
recall of positive:  0.8739889314601959
F1 score of positive:  0.8449863662087771

precision of negative:  0.8640486852681134
recall of negative:  0.8640486852681134
F1 score of negative:  0.8640486852681134


SVM-Light toolkit:

spam:

precision of positive:  0.9880853091862266
recall of positive:  0.9018051326663767
F1 score of positive:  0.9429757234635284

precision of negative:  0.8906779661016949
recall of negative:  0.8906779661016949
F1 score of negative:  0.8906779661016949

sentiment:

precision of positive:  0.8335733388191731
recall of positive:  0.8624946785866326
F1 score of positive:  0.8477874254629146

precision of negative:  0.8568896765618077
recall of negative:  0.8568896765618077
F1 score of negative:  0.8568896765618077


MegaM toolkit:

spam test:

precision of positive:  0.9878888625029684
recall of positive:  0.9848484848484849
F1 score of positive:  0.986366330764671

precision of negative:  0.9844490341392298
recall of negative:  0.9844490341392298
F1 score of negative:  0.9844490341392298

sentiment test:

precision of positive:  0.8320246179966044
recall of positive:  0.8211331029427165
F1 score of positive:  0.8265429821325041

precision of negative:  0.816856101222389
recall of negative:  0.816856101222389
F1 score of negative:  0.8168561012223892

1.
When 75% of the development data is used to train the model, and the rest for testing:
MegaM performs the best in spam test.
SVM-light performs the best in sentiment test.


2.


SVM drops more compared to other two.

MegaM is more robust compared to other two given a smaller training set.

Naive Bayes doesn't drop much.
SVM drops by 0.10


There is a huge difference between naive Bayes and Sentiment


spam model:

First, I created a folder emails_input, and moved all the enron folders (which were already unzipped) into the folder emails_input.
Then, I 